"""Career_Counselor_TinyLlama_Finetune (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ui6WzsZmlouumh8qR5ND-QmO_6ltZNYA

#  Career Counselor Bot ‚Äî TinyLlama 1.1B

## ‚úÖ Step 1 ‚Äî Check GPU
"""

import torch

print(f"\nGPU Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    print("GPU: None (CPU-only mode)")

"""## ‚úÖ Step 2 ‚Äî Install Libraries"""

print('‚úÖ Done!')

"""## ‚úÖ Step 3 ‚Äî Upload Dataset"""

import json
from datasets import Dataset

with open('career_qa_dataset.json', 'r') as f:
    raw_data = json.load(f)

system_prompt = 'You are an expert career counselor specializing in helping students in India choose the right career path. Always be encouraging, specific, and data-driven in your responses.'

# TinyLlama chat format
def format_example(item):
    text = (
        f"<|system|>\n{system_prompt}</s>\n"
        f"<|user|>\n{item['question']}</s>\n"
        f"<|assistant|>\n{item['answer']}</s>"
    )
    return {'text': text}

formatted = [format_example(item) for item in raw_data]
dataset = Dataset.from_list(formatted)

print(f'‚úÖ Loaded {len(dataset)} examples!')
print('\n--- Sample ---')
print(dataset[0]['text'][:400])

"""## ‚úÖ Step 5 ‚Äî Load TinyLlama Model

"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
print('‚è≥ Loading TinyLlama...')

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map='auto'
)
model.config.use_cache = False

print('‚úÖ TinyLlama loaded!')
print(f'Memory used: {torch.cuda.memory_allocated()/1e9:.2f} GB')

"""## ‚úÖ Step 6 ‚Äî Set Up LoRA"""

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=['q_proj', 'v_proj'],
    lora_dropout=0.05,
    bias='none',
    task_type='CAUSAL_LM'
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
print('\n‚úÖ LoRA ready!')

"""## ‚úÖ Step 7 ‚Äî Train!"""

from trl import SFTTrainer, SFTConfig

training_args = SFTConfig(
    output_dir='./tinyllama-career-counselor',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=2e-4,
    fp16=True,
    bf16=False,
    logging_steps=10,
    save_steps=50,
    dataset_text_field='text',
    warmup_steps=10,
    lr_scheduler_type='cosine',
    optim='paged_adamw_8bit',
    report_to='none'
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=training_args,
    max_seq_length=512,
)

print('üöÄ Training started!')
trainer.train()
print('üéâ Training complete!')

"""## ‚úÖ Step 8 ‚Äî Save"""

trainer.save_model('./tinyllama-career-counselor')
tokenizer.save_pretrained('./tinyllama-career-counselor')
print('Saved!')

"""## ‚úÖ Step 9 ‚Äî Test Your Bot üéì"""

from transformers import pipeline

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=200,
    temperature=0.4,
    top_p=0.9,
    repetition_penalty=1.2,
)

def ask_bot(question):
    prompt = (
        "<|system|>\n"
        "You are a professional career counselor.\n"
        "Always answer in full sentences with explanation.\n"
        "Never reply with a single word.\n"
        "</s>\n"
        f"<|user|>\n{question}</s>\n"
        "<|assistant|>\n"
    )

    result = pipe(prompt)[0]['generated_text']
    return result.split('<|assistant|>')[-1].strip()

questions = [
    'I am a Science PCM student. What career should I choose?',
    'I love coding. Which engineering branch is best?',
    'What is the scope of Data Science in India?'
]

for q in questions:
    print(f'\nüë§ {q}')
    print(f'ü§ñ {ask_bot(q)}')
    print('-' * 60)

"""## ‚úÖ Step 10 ‚Äî Download Model"""

import shutil
shutil.make_archive('tinyllama-career-counselor', 'zip', './tinyllama-career-counselor')
print('‚úÖ Downloaded! üéâ')