# -*- coding: utf-8 -*-
"""Career_Counselor_TinyLlama_Finetune (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ui6WzsZmlouumh8qR5ND-QmO_6ltZNYA

#  Career Counselor Bot â€” TinyLlama 1.1B

## âœ… Step 1 â€” Check GPU
"""

!nvidia-smi
import torch
print(f'\nGPU Available: {torch.cuda.is_available()}')
print(f'GPU: {torch.cuda.get_device_name(0)}')
print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')

"""## âœ… Step 2 â€” Install Libraries"""

!pip install -q transformers datasets peft trl accelerate bitsandbytes
print('âœ… Done!')

"""## âœ… Step 3 â€” Upload Dataset"""

from google.colab import files
print('ğŸ“‚ Upload your career_qa_dataset.json')
uploaded = files.upload()
print('âœ… Uploaded!')

"""## âœ… Step 4 â€” Load and Format Dataset"""

import json
from datasets import Dataset

with open('career_qa_dataset.json', 'r') as f:
    raw_data = json.load(f)

system_prompt = 'You are an expert career counselor specializing in helping students in India choose the right career path. Always be encouraging, specific, and data-driven in your responses.'

# TinyLlama chat format
def format_example(item):
    text = (
        f"<|system|>\n{system_prompt}</s>\n"
        f"<|user|>\n{item['question']}</s>\n"
        f"<|assistant|>\n{item['answer']}</s>"
    )
    return {'text': text}

formatted = [format_example(item) for item in raw_data]
dataset = Dataset.from_list(formatted)

print(f'âœ… Loaded {len(dataset)} examples!')
print('\n--- Sample ---')
print(dataset[0]['text'][:400])

"""## âœ… Step 5 â€” Load TinyLlama Model

"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
print('â³ Loading TinyLlama...')

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map='auto'
)
model.config.use_cache = False

print('âœ… TinyLlama loaded!')
print(f'Memory used: {torch.cuda.memory_allocated()/1e9:.2f} GB')

"""## âœ… Step 6 â€” Set Up LoRA"""

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=['q_proj', 'v_proj'],
    lora_dropout=0.05,
    bias='none',
    task_type='CAUSAL_LM'
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
print('\nâœ… LoRA ready!')

"""## âœ… Step 7 â€” Train!"""

from trl import SFTTrainer, SFTConfig

training_args = SFTConfig(
    output_dir='./tinyllama-career-counselor',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=2e-4,
    fp16=False,
    bf16=False,
    logging_steps=10,
    save_steps=50,
    dataset_text_field='text',
    warmup_steps=10,
    lr_scheduler_type='cosine',
    optim='paged_adamw_8bit',
    report_to='none'
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=training_args
)

print('ğŸš€ Training started!')
trainer.train()
print('ğŸ‰ Training complete!')

"""## âœ… Step 8 â€” Save"""

trainer.save_model('./tinyllama-career-counselor')
tokenizer.save_pretrained('./tinyllama-career-counselor')
print('âœ… Saved!')

"""## âœ… Step 9 â€” Test Your Bot ğŸ“"""

from transformers import pipeline

def ask_bot(question):
    prompt = (
        f"<|system|>\nYou are an expert career counselor for Indian students.</s>\n"
        f"<|user|>\n{question}</s>\n"
        f"<|assistant|>\n"
    )
    pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=250)
    result = pipe(prompt)[0]['generated_text']
    return result.split('<|assistant|>')[-1].strip()

questions = [
    'I am a Science PCM student. What career should I choose?',
    'I love coding. Which engineering branch is best?',
    'What is the scope of Data Science in India?'
]

for q in questions:
    print(f'\nğŸ‘¤ {q}')
    print(f'ğŸ¤– {ask_bot(q)}')
    print('-' * 60)

"""## âœ… Step 10 â€” Download Model"""

import shutil
from google.colab import files
shutil.make_archive('tinyllama-career-counselor', 'zip', './tinyllama-career-counselor')
files.download('tinyllama-career-counselor.zip')
print('âœ… Downloaded! ğŸ‰')